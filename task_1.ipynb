{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task 1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLBBP_l2rTFJ",
        "outputId": "c2c74603-efde-4fb9-e8d2-414ab7562970"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from os.path import join\n",
        "import string\n",
        "import re\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps0Cs-NTRJkU",
        "outputId": "70f2c3ac-5d1e-423f-afb9-4d5091ff08bd"
      },
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load('word2vec-google-news-300')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwYflr9blTjT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf16fda-e274-4d7d-e80c-ba4cd68b4933"
      },
      "source": [
        "\n",
        "# 1. read data\n",
        "\n",
        "dfl = pd.read_csv(join('data', \"/content/ltable.csv\"))\n",
        "dfr = pd.read_csv(join('data', \"/content/rtable.csv\"))\n",
        "train = pd.read_csv(join('data', \"/content/train.csv\"))\n",
        "\n",
        "# data preprocessing\n",
        "\n",
        "dfr.category = dfr.category.replace(np.nan,dfr['category'].value_counts().index[0])\n",
        "dfr.price = dfr.price.replace(np.nan,np.mean(dfr.price))\n",
        "\n",
        "dfl.category = dfl.category.replace(np.nan,dfl['category'].value_counts().index[0])\n",
        "dfl.price = dfl.price.replace(np.nan,np.mean(dfl.price))\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "ps = nltk.PorterStemmer()\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "\n",
        "tlt = []\n",
        "cat = []\n",
        "for row in range(len(dfr)):\n",
        "  sen = dfr.title[row]\n",
        "  sen = \"\".join([i.lower() for i in sen if i not in string.punctuation])\n",
        "  sen = nltk.tokenize.word_tokenize(sen)\n",
        "  sen = [i for i in sen if i not in stopwords]\n",
        "  sen = [wn.lemmatize(i) for i in sen]\n",
        "  sen = [i for i in sen if not re.search(r'\\d+',i)]\n",
        "  sen = \" \".join(sen)\n",
        "  tlt.append(sen)\n",
        "\n",
        "  sen = dfr.category[row]\n",
        "  sen = \"\".join([i.lower() for i in sen if i not in string.punctuation])\n",
        "  sen = nltk.tokenize.word_tokenize(sen)\n",
        "  \n",
        "  sen = [i for i in sen if i not in stopwords]\n",
        "  sen = [wn.lemmatize(i) for i in sen]\n",
        "  sen = [i for i in sen if not re.search(r'\\d+',i)]\n",
        "  sen = \" \".join(sen)\n",
        "  cat.append(sen)\n",
        "\n",
        "dfr.title = tlt\n",
        "dfr.category = cat\n",
        "tlt = []\n",
        "cat = []\n",
        "\n",
        "for row in range(len(dfl)):\n",
        "  sen = dfl.title[row]\n",
        "  sen = \"\".join([i.lower() for i in sen if i not in string.punctuation])\n",
        "  sen = nltk.tokenize.word_tokenize(sen)\n",
        "  sen = [i for i in sen if i not in stopwords]\n",
        "  sen = [wn.lemmatize(i) for i in sen]\n",
        "  sen = [i for i in sen if not re.search(r'\\d+',i)]\n",
        "  sen = \" \".join(sen)\n",
        "  tlt.append(sen)\n",
        "\n",
        "  sen = dfl.category[row]\n",
        "  sen = \"\".join([i.lower() for i in sen if i not in string.punctuation])\n",
        "  sen = nltk.tokenize.word_tokenize(sen)\n",
        "  \n",
        "  sen = [i for i in sen if i not in stopwords]\n",
        "  sen = [wn.lemmatize(i) for i in sen]\n",
        "  sen = [i for i in sen if not re.search(r'\\d+',i)]\n",
        "  sen = \" \".join(sen)\n",
        "  cat.append(sen)\n",
        "  print(row/len(dfl),end = '\\r')\n",
        "dfl.title = tlt\n",
        "dfl.category = cat\n",
        "\n",
        "\n",
        "\n",
        "# 2. blocking\n",
        "def pairs2LR(ltable, rtable, candset):\n",
        "    ltable.index = ltable.id\n",
        "    rtable.index = rtable.id\n",
        "    pairs = np.array(candset)\n",
        "    tpls_l = ltable.loc[pairs[:, 0], :]\n",
        "    tpls_r = rtable.loc[pairs[:, 1], :]\n",
        "    tpls_l.columns = [col + \"_l\" for col in tpls_l.columns]\n",
        "    tpls_r.columns = [col + \"_r\" for col in tpls_r.columns]\n",
        "    tpls_l.reset_index(inplace=True, drop=True)\n",
        "    tpls_r.reset_index(inplace=True, drop=True)\n",
        "    LR = pd.concat([tpls_l, tpls_r], axis=1)\n",
        "    return LR\n",
        "\n",
        "\n",
        "def block_by_brand(ltable, rtable):\n",
        "    # ensure brand is str\n",
        "    ltable['brand'] = ltable['brand'].astype(str)\n",
        "    rtable['brand'] = rtable['brand'].astype(str)\n",
        "\n",
        "    # get all brands\n",
        "    brands_l = set(ltable[\"brand\"].values)\n",
        "    brands_r = set(rtable[\"brand\"].values)\n",
        "    brands = brands_l.union(brands_r)\n",
        "\n",
        "    # map each brand to left ids and right ids\n",
        "    brand2ids_l = {b.lower(): [] for b in brands}\n",
        "    brand2ids_r = {b.lower(): [] for b in brands}\n",
        "    for i, x in ltable.iterrows():\n",
        "        brand2ids_l[x[\"brand\"].lower()].append(x[\"id\"])\n",
        "    for i, x in rtable.iterrows():\n",
        "        brand2ids_r[x[\"brand\"].lower()].append(x[\"id\"])\n",
        "\n",
        "    # put id pairs that share the same brand in candidate set\n",
        "    candset = []\n",
        "    for brd in brands:\n",
        "        l_ids = brand2ids_l[brd]\n",
        "        r_ids = brand2ids_r[brd]\n",
        "        for i in range(len(l_ids)):\n",
        "            for j in range(len(r_ids)):\n",
        "                candset.append([l_ids[i], r_ids[j]])\n",
        "    return candset\n",
        "\n",
        "# blocking to reduce the number of pairs to be compared\n",
        "candset = block_by_brand(dfl, dfr)\n",
        "print(\"number of pairs originally\", dfl.shape[0] * dfr.shape[0])\n",
        "print(\"number of pairs after blocking\",len(candset))\n",
        "candset_df = pairs2LR(dfl, dfr, candset)\n",
        "\n",
        "\n",
        "\n",
        "# 3. Feature engineering\n",
        "\n",
        "def jaccard_similarity(row, attr):\n",
        "    x = set(row[attr + \"_l\"].lower().split())\n",
        "    y = set(row[attr + \"_r\"].lower().split())\n",
        "    return len(x.intersection(y)) / max(len(x), len(y))\n",
        "\n",
        "\n",
        "def WordMovers_distance(row, attr):\n",
        "    x = row[attr + \"_l\"].lower()\n",
        "    y = row[attr + \"_r\"].lower()\n",
        "    return model.wmdistance(x, y)\n",
        "\n",
        "def feature_engineering(LR):\n",
        "    LR = LR.astype(str)\n",
        "    attrs = [\"title\", \"category\", \"brand\", \"modelno\", \"price\"]\n",
        "    features = []\n",
        "    for attr in attrs:\n",
        "        j_sim = LR.apply(jaccard_similarity, attr=attr, axis=1)\n",
        "        l_dist = LR.apply(WordMovers_distance, attr=attr, axis=1)\n",
        "        features.append(j_sim)\n",
        "        features.append(l_dist)\n",
        "    features = np.array(features).T\n",
        "    return features\n",
        "candset_features = feature_engineering(candset_df)\n",
        "\n",
        "# also perform feature engineering to the training set\n",
        "training_pairs = list(map(tuple, train[[\"ltable_id\", \"rtable_id\"]].values))\n",
        "training_df = pairs2LR(dfl, dfr, training_pairs)\n",
        "training_features = feature_engineering(training_df)\n",
        "training_label = train.label.values\n",
        "\n",
        "# 4. Model training and prediction\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "rf = RandomForestClassifier(class_weight=\"balanced\", random_state=0)\n",
        "\n",
        "for i  in range(len(training_features)):\n",
        "  for j in range(len(training_features[i])):\n",
        "    if training_features[i][j] == np.inf:\n",
        "      training_features[i][j] = -1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(training_features, training_label, test_size=0.2, random_state=0)\n",
        "rf.fit(X_train,y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# metrics \n",
        "print(f\"f1 score : {f1_score(y_test, y_pred, zero_division=1)}\")\n",
        "print(f\"recall score : {recall_score(y_test, y_pred, average='macro')}\")\n",
        "print(f\"precision score : {precision_score(y_test, y_pred, average='macro')}\")\n",
        "\n",
        "# 5. output\n",
        "for i  in range(len(candset_features)):\n",
        "  for j in range(len(candset_features[i])):\n",
        "    if candset_features[i][j] == np.inf:\n",
        "      candset_features[i][j] = -1\n",
        "\n",
        "rf.fit(training_features,training_label)\n",
        "y_pred = rf.predict(candset_features)\n",
        "matching_pairs = candset_df.loc[y_pred == 1, [\"id_l\", \"id_r\"]]\n",
        "matching_pairs = list(map(tuple, matching_pairs.values))\n",
        "\n",
        "matching_pairs_in_training = training_df.loc[training_label == 1, [\"id_l\", \"id_r\"]]\n",
        "matching_pairs_in_training = set(list(map(tuple, matching_pairs_in_training.values)))\n",
        "\n",
        "pred_pairs = [pair for pair in matching_pairs if\n",
        "              pair not in matching_pairs_in_training]  # remove the matching pairs already in training\n",
        "pred_pairs = np.array(pred_pairs)\n",
        "pred_df = pd.DataFrame(pred_pairs, columns=[\"ltable_id\", \"rtable_id\"])\n",
        "pred_df.to_csv(\"output.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 score : 0.7210884353741497\n",
            "recall score : 0.7950085718866784\n",
            "precision score : 0.9377882714693608\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}